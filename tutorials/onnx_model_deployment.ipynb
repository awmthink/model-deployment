{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.模型部署简介\n",
    "\n",
    "模型部署： 算法模型 -->  应用程序\n",
    "\n",
    "希望通过本教程，带领大家学会如何把自己的 PyTorch 模型部署到 ONNX Runtime/TensorRT 上。\n",
    "\n",
    "* 中间表示 ONNX 的定义标准\n",
    "* PyTorch 模型转换到 ONNX 模型的方法\n",
    "* 推理引擎 ONNX Runtime、TensorRT 的使用方法\n",
    "* 部署流水线 PyTorch - ONNX - ONNX Runtime/TensorRT 的示例及常见部署问题的解决方法\n",
    "* MMDeploy C/C++ 推理 SDK\n",
    "\n",
    "\n",
    "软件工程领域的“部署”：把开发完毕的软件投入使用的过程，包括环境配置、软件安装等。对于深度学习模型而言，模型部署指让训练好的模型在特定的环境中运行的过程。这过程中需要解决：环境适配、运行性能等问题。\n",
    "\n",
    "部署流水线：\n",
    "\n",
    "1. 训练：使用深度学习框架训练模型的参数，训练框架包括了：Pytorch、TensorFlow、Caffe、MXNet等\n",
    "2. 中间表示（优化）：将不同框架下训练的模型结构定义和权重参数，转换为中间表达，常见的中间表达有：ONNX、TorchScript、Caffe等\n",
    "3. 推理引擎（运行）：将中间表示转化为特定的优化的推理引擎的专用格式。常见的推理引擎有：TensorRT、ONNXRuntime、PPL、NCNN、OpenVINO等。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.部署第一个模型\n",
    "\n",
    "Pytorch Module -> ONNX -> ONNX Runtime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 定义一个简单的图像超分的模型\n",
    "\n",
    "该超分模型由一个上采样层和3层卷积层构成。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "import requests\n",
    "import torch\n",
    "import torch.onnx\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "class SuperResolutionNet(nn.Module):\n",
    "    def __init__(self, upscale_factor):\n",
    "        super().__init__()\n",
    "        self.upsampler = nn.Upsample(\n",
    "            scale_factor=upscale_factor, mode=\"bicubic\", align_corners=False\n",
    "        )\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=9, padding=4)\n",
    "        self.conv2 = nn.Conv2d(64, 32, kernel_size=1, padding=0)  # 压缩channel\n",
    "        self.conv3 = nn.Conv2d(32, 3, kernel_size=5, padding=2)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.upsampler(x)\n",
    "        out = self.relu(self.conv1(out))\n",
    "        out = self.relu(self.conv2(out))\n",
    "        out = self.conv3(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "upsampler: [1, 3, 224, 224] => [1, 3, 448, 448]\n",
      "conv1: [1, 3, 448, 448] => [1, 64, 448, 448]\n",
      "conv2: [1, 64, 448, 448] => [1, 32, 448, 448]\n",
      "conv3: [1, 32, 448, 448] => [1, 3, 448, 448]\n"
     ]
    }
   ],
   "source": [
    "model = SuperResolutionNet(2.0)\n",
    "x = torch.randn((1, 3, 224, 224))\n",
    "for modules in model.named_modules():\n",
    "    if modules[0] == \"\" or modules[0] == \"relu\":\n",
    "        continue\n",
    "    shape = x.shape\n",
    "    x = model.get_submodule(modules[0])(x)\n",
    "    print(f\"{modules[0]}: {list(shape)} => {list(x.shape)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 加载训练好的模型权重"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "odict_keys(['generator.conv1.weight', 'generator.conv1.bias', 'generator.conv2.weight', 'generator.conv2.bias', 'generator.conv3.weight', 'generator.conv3.bias'])\n",
      "odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'conv3.weight', 'conv3.bias'])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "urls = [\n",
    "    \"https://download.openmmlab.com/mmediting/restorers/srcnn/srcnn_x4k915_1x16_1000k_div2k_20200608-4186f232.pth\",\n",
    "    \"https://raw.githubusercontent.com/open-mmlab/mmediting/master/tests/data/face/000001.png\",\n",
    "]\n",
    "\n",
    "names = [\"srcnn.pth\", \"face.png\"]\n",
    "\n",
    "for url, name in zip(urls, names):\n",
    "    if not os.path.exists(name):\n",
    "        open(name, \"wb\").write(requests.get(url).content)\n",
    "\n",
    "# 加载离线权重，并对key进行适配\n",
    "state_dicts = torch.load(\"srcnn.pth\")[\"state_dict\"]\n",
    "print(state_dicts.keys())\n",
    "for old_key in list(state_dicts.keys()):\n",
    "    new_key = old_key.replace(\"generator.\", \"\", 1)\n",
    "    state_dicts[new_key] = state_dicts.pop(old_key)\n",
    "print(state_dicts.keys())\n",
    "srnn_model = SuperResolutionNet(upscale_factor=3)\n",
    "srnn_model.load_state_dict(state_dict=state_dicts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 使用Pytorch进行推理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_image = cv2.imread(\"./face.png\").astype(np.float32)\n",
    "input_image = np.transpose(input_image, (2, 0, 1))  # H,W,C -> C,H,W\n",
    "input_image = np.expand_dims(input_image, 0)  # C,H,W -> 1,C,H,W\n",
    "\n",
    "torch_output = srnn_model(torch.from_numpy(input_image)).detach().numpy()\n",
    "torch_output = np.squeeze(torch_output, 0)\n",
    "torch_output = np.clip(torch_output, 0, 255)\n",
    "torch_output = np.transpose(torch_output, (1, 2, 0)).astype(np.uint8)\n",
    "\n",
    "cv2.imwrite(\"./outputs/face_torch_1.png\", torch_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 导出为ONNX格式的模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============= Diagnostic Run torch.onnx.export version 2.0.1+cu118 =============\n",
      "verbose: False, log level: Level.ERROR\n",
      "======================= 0 NONE 0 NOTE 0 WARNING 0 ERROR ========================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import onnx\n",
    "\n",
    "x = torch.randn(1, 3, 256, 256)\n",
    "with torch.no_grad():\n",
    "    torch.onnx.export(\n",
    "        model=srnn_model,\n",
    "        args=x,\n",
    "        f=\"./outputs/srcnn1.onnx\",\n",
    "        opset_version=11,\n",
    "        input_names=[\"input\"],\n",
    "        output_names=[\"output\"],\n",
    "    )\n",
    "\n",
    "# 使用onnx来加载序列化的模型，加载出来的就是一个ModelProto对象\n",
    "onnx_model = onnx.load(\"./outputs/srcnn1.onnx\")\n",
    "# 检查ModelProto是否正常\n",
    "onnx.checker.check_model(onnx_model, full_check=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 使用Netron来查看Onnx文件\n",
    "\n",
    "地址：[netron.app](https://netron.app/)\n",
    "\n",
    "<div align=\"center\">\n",
    "  <img src=\"./assets/srcnn.onnx.svg\" height=\"600\"/> </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.6 使用OnnxRuntime进行推理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnxruntime\n",
    "\n",
    "ort_session = onnxruntime.InferenceSession(\"./outputs/srcnn1.onnx\")\n",
    "ort_inputs = {\"input\": input_image}\n",
    "\n",
    "# 输入输出都是numpy.ndarry\n",
    "ort_output = ort_session.run(output_names=[\"output\"], input_feed=ort_inputs)[0]\n",
    "\n",
    "ort_output = np.squeeze(ort_output, 0)\n",
    "ort_output = np.clip(ort_output, 0, 255)\n",
    "ort_output = np.transpose(ort_output, (1, 2, 0)).astype(np.uint8)\n",
    "\n",
    "successed = cv2.imwrite(\"./outputs/face_ort_1.png\", ort_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. 解决模型部署中的难题"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上面部署的模型，十分顺利，但是它有一个缺陷，图片的放大倍数被写死在模型结构中了，我们在推理时，无法动态的调整。\n",
    "\n",
    "另外上面模型部署的简单也主要是因为SCRNN结构比较简单，只包括了：`Upsample`、`Conv2d`、`Relu`这3个标准的算子。这些算法在各个中间表示和推理引擎上已经得到完美的支持。如果模型的操作稍微复杂一点，我们可能就要为兼容模型而付出大量的功夫了。在实际部署时，我们可能会遇到几下几类的困难：\n",
    "\n",
    "* 模型的动态化。出于性能的考虑，各推理框架都默认模型的输入形状、输出形状、结构是静态的。而为了让模型的泛化性更强，部署时需要在尽可能不影响原有逻辑的前提下，让模型的输入输出或是结构动态化。\n",
    "* 新算子的实现。深度学习技术日新月异，提出新算子的速度往往快于 ONNX 维护者支持的速度。为了部署最新的模型，部署工程师往往需要自己在 ONNX 和推理引擎中支持新算子。\n",
    "* 中间表示与推理引擎的兼容问题。由于各推理引擎的实现不同，对 ONNX 难以形成统一的支持。为了确保模型在不同的推理引擎中有同样的运行效果，部署工程师往往得为某个推理引擎定制模型代码，这为模型部署引入了许多工作量。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 SRCNN支持动态设置放大倍数\n",
    "\n",
    "下面我们对前面的`SRCNN`网络结构进行修改，将上采样写在`forward`阶段，调用`interpolate`函数来实现。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SuperResolutionNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4))\n",
       "  (conv2): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "  (conv3): Conv2d(32, 3, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "  (relu): ReLU()\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class SuperResolutionNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=9, padding=4)\n",
    "        self.conv2 = nn.Conv2d(64, 32, kernel_size=1, padding=0)  # 压缩channel\n",
    "        self.conv3 = nn.Conv2d(32, 3, kernel_size=5, padding=2)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x, upscale_factor):\n",
    "        out = F.interpolate(\n",
    "            x, scale_factor=upscale_factor, mode=\"bicubic\", align_corners=False\n",
    "        )\n",
    "        out = self.relu(self.conv1(out))\n",
    "        out = self.relu(self.conv2(out))\n",
    "        out = self.conv3(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "torch_model = SuperResolutionNet()\n",
    "torch_model.load_state_dict(state_dicts)\n",
    "torch_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "upscale_factor = 3.0\n",
    "torch_output = (\n",
    "    torch_model(torch.from_numpy(input_image), upscale_factor).detach().numpy()\n",
    ")\n",
    "torch_output = np.squeeze(torch_output, 0)\n",
    "torch_output = np.clip(torch_output, 0, 255)\n",
    "torch_output = np.transpose(torch_output, (1, 2, 0)).astype(np.uint8)\n",
    "cv2.imwrite(\"./outputs/face_torch_2.png\", torch_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "由于pytorch本身具有动态性，所以上面的过程没有任何问题，我们在forward时，指定了放大倍数。现在我们来尝试将上面的模型转换为ONNX模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============= Diagnostic Run torch.onnx.export version 2.0.1+cu118 =============\n",
      "verbose: False, log level: Level.ERROR\n",
      "======================= 0 NONE 0 NOTE 0 WARNING 0 ERROR ========================\n",
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "upsample_bicubic2d() received an invalid combination of arguments - got (Tensor, NoneType, bool, list), but expected one of:\n * (Tensor input, tuple of ints output_size, bool align_corners, tuple of floats scale_factors)\n      didn't match because some of the arguments have invalid types: (Tensor, !NoneType!, bool, !list of [Tensor, Tensor]!)\n * (Tensor input, tuple of ints output_size, bool align_corners, float scales_h, float scales_w, *, Tensor out)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/yangyansheng/workspace/pyml/model-deployment/tutorial.ipynb Cell 21\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a225034313132227d/home/yangyansheng/workspace/pyml/model-deployment/tutorial.ipynb#X30sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a225034313132227d/home/yangyansheng/workspace/pyml/model-deployment/tutorial.ipynb#X30sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m     torch\u001b[39m.\u001b[39;49monnx\u001b[39m.\u001b[39;49mexport(\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a225034313132227d/home/yangyansheng/workspace/pyml/model-deployment/tutorial.ipynb#X30sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m         model\u001b[39m=\u001b[39;49mtorch_model,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a225034313132227d/home/yangyansheng/workspace/pyml/model-deployment/tutorial.ipynb#X30sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m         args\u001b[39m=\u001b[39;49m(x, \u001b[39m3\u001b[39;49m),\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a225034313132227d/home/yangyansheng/workspace/pyml/model-deployment/tutorial.ipynb#X30sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m         f\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m./outputs/srcnn2.onnx\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a225034313132227d/home/yangyansheng/workspace/pyml/model-deployment/tutorial.ipynb#X30sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m         opset_version\u001b[39m=\u001b[39;49m\u001b[39m11\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a225034313132227d/home/yangyansheng/workspace/pyml/model-deployment/tutorial.ipynb#X30sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m         input_names\u001b[39m=\u001b[39;49m[\u001b[39m\"\u001b[39;49m\u001b[39minput\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mfactor\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a225034313132227d/home/yangyansheng/workspace/pyml/model-deployment/tutorial.ipynb#X30sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m         output_names\u001b[39m=\u001b[39;49m[\u001b[39m\"\u001b[39;49m\u001b[39moutput\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a225034313132227d/home/yangyansheng/workspace/pyml/model-deployment/tutorial.ipynb#X30sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/d2l/lib/python3.8/site-packages/torch/onnx/utils.py:506\u001b[0m, in \u001b[0;36mexport\u001b[0;34m(model, args, f, export_params, verbose, training, input_names, output_names, operator_export_type, opset_version, do_constant_folding, dynamic_axes, keep_initializers_as_inputs, custom_opsets, export_modules_as_functions)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[39m@_beartype\u001b[39m\u001b[39m.\u001b[39mbeartype\n\u001b[1;32m    189\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mexport\u001b[39m(\n\u001b[1;32m    190\u001b[0m     model: Union[torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mModule, torch\u001b[39m.\u001b[39mjit\u001b[39m.\u001b[39mScriptModule, torch\u001b[39m.\u001b[39mjit\u001b[39m.\u001b[39mScriptFunction],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    206\u001b[0m     export_modules_as_functions: Union[\u001b[39mbool\u001b[39m, Collection[Type[torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mModule]]] \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    207\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    208\u001b[0m \u001b[39m    \u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"Exports a model into ONNX format.\u001b[39;00m\n\u001b[1;32m    209\u001b[0m \n\u001b[1;32m    210\u001b[0m \u001b[39m    If ``model`` is not a :class:`torch.jit.ScriptModule` nor a\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    503\u001b[0m \u001b[39m            All errors are subclasses of :class:`errors.OnnxExporterError`.\u001b[39;00m\n\u001b[1;32m    504\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 506\u001b[0m     _export(\n\u001b[1;32m    507\u001b[0m         model,\n\u001b[1;32m    508\u001b[0m         args,\n\u001b[1;32m    509\u001b[0m         f,\n\u001b[1;32m    510\u001b[0m         export_params,\n\u001b[1;32m    511\u001b[0m         verbose,\n\u001b[1;32m    512\u001b[0m         training,\n\u001b[1;32m    513\u001b[0m         input_names,\n\u001b[1;32m    514\u001b[0m         output_names,\n\u001b[1;32m    515\u001b[0m         operator_export_type\u001b[39m=\u001b[39;49moperator_export_type,\n\u001b[1;32m    516\u001b[0m         opset_version\u001b[39m=\u001b[39;49mopset_version,\n\u001b[1;32m    517\u001b[0m         do_constant_folding\u001b[39m=\u001b[39;49mdo_constant_folding,\n\u001b[1;32m    518\u001b[0m         dynamic_axes\u001b[39m=\u001b[39;49mdynamic_axes,\n\u001b[1;32m    519\u001b[0m         keep_initializers_as_inputs\u001b[39m=\u001b[39;49mkeep_initializers_as_inputs,\n\u001b[1;32m    520\u001b[0m         custom_opsets\u001b[39m=\u001b[39;49mcustom_opsets,\n\u001b[1;32m    521\u001b[0m         export_modules_as_functions\u001b[39m=\u001b[39;49mexport_modules_as_functions,\n\u001b[1;32m    522\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/d2l/lib/python3.8/site-packages/torch/onnx/utils.py:1548\u001b[0m, in \u001b[0;36m_export\u001b[0;34m(model, args, f, export_params, verbose, training, input_names, output_names, operator_export_type, export_type, opset_version, do_constant_folding, dynamic_axes, keep_initializers_as_inputs, fixed_batch_size, custom_opsets, add_node_names, onnx_shape_inference, export_modules_as_functions)\u001b[0m\n\u001b[1;32m   1545\u001b[0m     dynamic_axes \u001b[39m=\u001b[39m {}\n\u001b[1;32m   1546\u001b[0m _validate_dynamic_axes(dynamic_axes, model, input_names, output_names)\n\u001b[0;32m-> 1548\u001b[0m graph, params_dict, torch_out \u001b[39m=\u001b[39m _model_to_graph(\n\u001b[1;32m   1549\u001b[0m     model,\n\u001b[1;32m   1550\u001b[0m     args,\n\u001b[1;32m   1551\u001b[0m     verbose,\n\u001b[1;32m   1552\u001b[0m     input_names,\n\u001b[1;32m   1553\u001b[0m     output_names,\n\u001b[1;32m   1554\u001b[0m     operator_export_type,\n\u001b[1;32m   1555\u001b[0m     val_do_constant_folding,\n\u001b[1;32m   1556\u001b[0m     fixed_batch_size\u001b[39m=\u001b[39;49mfixed_batch_size,\n\u001b[1;32m   1557\u001b[0m     training\u001b[39m=\u001b[39;49mtraining,\n\u001b[1;32m   1558\u001b[0m     dynamic_axes\u001b[39m=\u001b[39;49mdynamic_axes,\n\u001b[1;32m   1559\u001b[0m )\n\u001b[1;32m   1561\u001b[0m \u001b[39m# TODO: Don't allocate a in-memory string for the protobuf\u001b[39;00m\n\u001b[1;32m   1562\u001b[0m defer_weight_export \u001b[39m=\u001b[39m (\n\u001b[1;32m   1563\u001b[0m     export_type \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m _exporter_states\u001b[39m.\u001b[39mExportTypes\u001b[39m.\u001b[39mPROTOBUF_FILE\n\u001b[1;32m   1564\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/d2l/lib/python3.8/site-packages/torch/onnx/utils.py:1113\u001b[0m, in \u001b[0;36m_model_to_graph\u001b[0;34m(model, args, verbose, input_names, output_names, operator_export_type, do_constant_folding, _disable_torch_constant_prop, fixed_batch_size, training, dynamic_axes)\u001b[0m\n\u001b[1;32m   1110\u001b[0m     args \u001b[39m=\u001b[39m (args,)\n\u001b[1;32m   1112\u001b[0m model \u001b[39m=\u001b[39m _pre_trace_quant_model(model, args)\n\u001b[0;32m-> 1113\u001b[0m graph, params, torch_out, module \u001b[39m=\u001b[39m _create_jit_graph(model, args)\n\u001b[1;32m   1114\u001b[0m params_dict \u001b[39m=\u001b[39m _get_named_param_dict(graph, params)\n\u001b[1;32m   1116\u001b[0m \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/d2l/lib/python3.8/site-packages/torch/onnx/utils.py:989\u001b[0m, in \u001b[0;36m_create_jit_graph\u001b[0;34m(model, args)\u001b[0m\n\u001b[1;32m    984\u001b[0m     graph \u001b[39m=\u001b[39m _C\u001b[39m.\u001b[39m_propagate_and_assign_input_shapes(\n\u001b[1;32m    985\u001b[0m         graph, flattened_args, param_count_list, \u001b[39mFalse\u001b[39;00m, \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m    986\u001b[0m     )\n\u001b[1;32m    987\u001b[0m     \u001b[39mreturn\u001b[39;00m graph, params, torch_out, \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 989\u001b[0m graph, torch_out \u001b[39m=\u001b[39m _trace_and_get_graph_from_model(model, args)\n\u001b[1;32m    990\u001b[0m _C\u001b[39m.\u001b[39m_jit_pass_onnx_lint(graph)\n\u001b[1;32m    991\u001b[0m state_dict \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mjit\u001b[39m.\u001b[39m_unique_state_dict(model)\n",
      "File \u001b[0;32m~/miniconda3/envs/d2l/lib/python3.8/site-packages/torch/onnx/utils.py:893\u001b[0m, in \u001b[0;36m_trace_and_get_graph_from_model\u001b[0;34m(model, args)\u001b[0m\n\u001b[1;32m    891\u001b[0m prev_autocast_cache_enabled \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mis_autocast_cache_enabled()\n\u001b[1;32m    892\u001b[0m torch\u001b[39m.\u001b[39mset_autocast_cache_enabled(\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m--> 893\u001b[0m trace_graph, torch_out, inputs_states \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mjit\u001b[39m.\u001b[39;49m_get_trace_graph(\n\u001b[1;32m    894\u001b[0m     model,\n\u001b[1;32m    895\u001b[0m     args,\n\u001b[1;32m    896\u001b[0m     strict\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    897\u001b[0m     _force_outplace\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    898\u001b[0m     _return_inputs_states\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    899\u001b[0m )\n\u001b[1;32m    900\u001b[0m torch\u001b[39m.\u001b[39mset_autocast_cache_enabled(prev_autocast_cache_enabled)\n\u001b[1;32m    902\u001b[0m warn_on_static_input_change(inputs_states)\n",
      "File \u001b[0;32m~/miniconda3/envs/d2l/lib/python3.8/site-packages/torch/jit/_trace.py:1268\u001b[0m, in \u001b[0;36m_get_trace_graph\u001b[0;34m(f, args, kwargs, strict, _force_outplace, return_inputs, _return_inputs_states)\u001b[0m\n\u001b[1;32m   1266\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(args, \u001b[39mtuple\u001b[39m):\n\u001b[1;32m   1267\u001b[0m     args \u001b[39m=\u001b[39m (args,)\n\u001b[0;32m-> 1268\u001b[0m outs \u001b[39m=\u001b[39m ONNXTracedModule(f, strict, _force_outplace, return_inputs, _return_inputs_states)(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1269\u001b[0m \u001b[39mreturn\u001b[39;00m outs\n",
      "File \u001b[0;32m~/miniconda3/envs/d2l/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/d2l/lib/python3.8/site-packages/torch/jit/_trace.py:127\u001b[0m, in \u001b[0;36mONNXTracedModule.forward\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    125\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mtuple\u001b[39m(out_vars)\n\u001b[0;32m--> 127\u001b[0m graph, out \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49m_C\u001b[39m.\u001b[39;49m_create_graph_by_tracing(\n\u001b[1;32m    128\u001b[0m     wrapper,\n\u001b[1;32m    129\u001b[0m     in_vars \u001b[39m+\u001b[39;49m module_state,\n\u001b[1;32m    130\u001b[0m     _create_interpreter_name_lookup_fn(),\n\u001b[1;32m    131\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstrict,\n\u001b[1;32m    132\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_force_outplace,\n\u001b[1;32m    133\u001b[0m )\n\u001b[1;32m    135\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_return_inputs:\n\u001b[1;32m    136\u001b[0m     \u001b[39mreturn\u001b[39;00m graph, outs[\u001b[39m0\u001b[39m], ret_inputs[\u001b[39m0\u001b[39m]\n",
      "File \u001b[0;32m~/miniconda3/envs/d2l/lib/python3.8/site-packages/torch/jit/_trace.py:118\u001b[0m, in \u001b[0;36mONNXTracedModule.forward.<locals>.wrapper\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_return_inputs_states:\n\u001b[1;32m    117\u001b[0m     inputs_states\u001b[39m.\u001b[39mappend(_unflatten(in_args, in_desc))\n\u001b[0;32m--> 118\u001b[0m outs\u001b[39m.\u001b[39mappend(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minner(\u001b[39m*\u001b[39;49mtrace_inputs))\n\u001b[1;32m    119\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_return_inputs_states:\n\u001b[1;32m    120\u001b[0m     inputs_states[\u001b[39m0\u001b[39m] \u001b[39m=\u001b[39m (inputs_states[\u001b[39m0\u001b[39m], trace_inputs)\n",
      "File \u001b[0;32m~/miniconda3/envs/d2l/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/d2l/lib/python3.8/site-packages/torch/nn/modules/module.py:1488\u001b[0m, in \u001b[0;36mModule._slow_forward\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1486\u001b[0m         recording_scopes \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m   1487\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1488\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1489\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m   1490\u001b[0m     \u001b[39mif\u001b[39;00m recording_scopes:\n",
      "\u001b[1;32m/home/yangyansheng/workspace/pyml/model-deployment/tutorial.ipynb Cell 21\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a225034313132227d/home/yangyansheng/workspace/pyml/model-deployment/tutorial.ipynb#X30sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x, upscale_factor):\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a225034313132227d/home/yangyansheng/workspace/pyml/model-deployment/tutorial.ipynb#X30sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m     out \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39;49minterpolate(\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a225034313132227d/home/yangyansheng/workspace/pyml/model-deployment/tutorial.ipynb#X30sdnNjb2RlLXJlbW90ZQ%3D%3D?line=14'>15</a>\u001b[0m         x, scale_factor\u001b[39m=\u001b[39;49mupscale_factor, mode\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mbicubic\u001b[39;49m\u001b[39m\"\u001b[39;49m, align_corners\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a225034313132227d/home/yangyansheng/workspace/pyml/model-deployment/tutorial.ipynb#X30sdnNjb2RlLXJlbW90ZQ%3D%3D?line=15'>16</a>\u001b[0m     )\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a225034313132227d/home/yangyansheng/workspace/pyml/model-deployment/tutorial.ipynb#X30sdnNjb2RlLXJlbW90ZQ%3D%3D?line=16'>17</a>\u001b[0m     out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrelu(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv1(out))\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a225034313132227d/home/yangyansheng/workspace/pyml/model-deployment/tutorial.ipynb#X30sdnNjb2RlLXJlbW90ZQ%3D%3D?line=17'>18</a>\u001b[0m     out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrelu(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv2(out))\n",
      "File \u001b[0;32m~/miniconda3/envs/d2l/lib/python3.8/site-packages/torch/nn/functional.py:3967\u001b[0m, in \u001b[0;36minterpolate\u001b[0;34m(input, size, scale_factor, mode, align_corners, recompute_scale_factor, antialias)\u001b[0m\n\u001b[1;32m   3965\u001b[0m     \u001b[39mif\u001b[39;00m antialias:\n\u001b[1;32m   3966\u001b[0m         \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39m_C\u001b[39m.\u001b[39m_nn\u001b[39m.\u001b[39m_upsample_bicubic2d_aa(\u001b[39minput\u001b[39m, output_size, align_corners, scale_factors)\n\u001b[0;32m-> 3967\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49m_C\u001b[39m.\u001b[39;49m_nn\u001b[39m.\u001b[39;49mupsample_bicubic2d(\u001b[39minput\u001b[39;49m, output_size, align_corners, scale_factors)\n\u001b[1;32m   3969\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39minput\u001b[39m\u001b[39m.\u001b[39mdim() \u001b[39m==\u001b[39m \u001b[39m3\u001b[39m \u001b[39mand\u001b[39;00m mode \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mbilinear\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m   3970\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mNotImplementedError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mGot 3D input, but bilinear mode needs 4D input\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: upsample_bicubic2d() received an invalid combination of arguments - got (Tensor, NoneType, bool, list), but expected one of:\n * (Tensor input, tuple of ints output_size, bool align_corners, tuple of floats scale_factors)\n      didn't match because some of the arguments have invalid types: (Tensor, !NoneType!, bool, !list of [Tensor, Tensor]!)\n * (Tensor input, tuple of ints output_size, bool align_corners, float scales_h, float scales_w, *, Tensor out)\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    torch.onnx.export(\n",
    "        model=torch_model,\n",
    "        args=(x, 3),\n",
    "        f=\"./outputs/srcnn2.onnx\",\n",
    "        opset_version=11,\n",
    "        input_names=[\"input\", \"factor\"],\n",
    "        output_names=[\"output\"],\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上面报错的原因是，在导出到ONXX模型时，模型的输入参数的类型必须全部是torch.Tensor。而我们传入的第二个参数“3”是一个整形变量。这不符合Pytorch转ONNX的规定。现在我们可以尝试修改一下模型："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3137538/646510743.py:12: TracerWarning: Converting a tensor to a Python number might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  x, scale_factor=upscale_factor.item(), mode=\"bicubic\", align_corners=False\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============= Diagnostic Run torch.onnx.export version 2.0.1+cu118 =============\n",
      "verbose: False, log level: Level.ERROR\n",
      "======================= 0 NONE 0 NOTE 0 WARNING 0 ERROR ========================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class SuperResolutionNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=9, padding=4)\n",
    "        self.conv2 = nn.Conv2d(64, 32, kernel_size=1, padding=0)  # 压缩channel\n",
    "        self.conv3 = nn.Conv2d(32, 3, kernel_size=5, padding=2)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x, upscale_factor):\n",
    "        out = F.interpolate(\n",
    "            x, scale_factor=upscale_factor.item(), mode=\"bicubic\", align_corners=False\n",
    "        )\n",
    "        out = self.relu(self.conv1(out))\n",
    "        out = self.relu(self.conv2(out))\n",
    "        out = self.conv3(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "torch_model = SuperResolutionNet()\n",
    "torch_model.load_state_dict(state_dicts)\n",
    "torch_model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    torch.onnx.export(\n",
    "        model=torch_model,\n",
    "        args=(x, torch.tensor(3)),\n",
    "        f=\"./outputs/srcnn2.onnx\",\n",
    "        opset_version=11,\n",
    "        input_names=[\"input\", \"factor\"],\n",
    "        output_names=[\"output\"],\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "转换成功了，但报了一个Warning，意思是虽然转成功了，但在转换过程中，发现了把一个Tensor转换为了一个Python number，这会导致导出的ONNX模型，把这个值视为一个常量。也就是说导出的模型，我们还是无法动态的设置upscale_factor。\n",
    "\n",
    "如果我们在netron.app中加载刚才导出来的ONNX文件，会发现，还是只有一个输入。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 自定义ONNX算子解决问题\n",
    "\n",
    "仔细观察 Netron 上可视化出的 ONNX 模型，可以发现在 PyTorch 中无论是使用最早的 nn.Upsample，还是后来的 interpolate，PyTorch 里的插值操作最后都会转换成 ONNX 定义的 Resize 操作。也就是说，所谓 PyTorch 转 ONNX，实际上就是把每个 PyTorch 的操作映射成了 ONNX 定义的算子。\n",
    "\n",
    "其中`Resize`算子的输入是：`X`、`roi`、`scales`。`scales`是一个长度为4的一维张量，其内容为`[1,1,3,3]`代表的是各个维度的缩放系数。下面我们来尝试自定义一个pytorch的算子，希望把它映射到ONXX的Resize算子上。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NewInterpolate(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, input, scales):\n",
    "        scales = scales.tolist()[-2:]\n",
    "        return F.interpolate(\n",
    "            input, scale_factor=scales, mode=\"bicubic\", align_corners=False\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def symbolic(g, input, scales):\n",
    "        return g.op(\n",
    "            \"Resize\",\n",
    "            input,  # 第一个参数 X\n",
    "            g.op(\n",
    "                \"Constant\", value_t=torch.tensor([], dtype=torch.float32)\n",
    "            ),  # 第二个参数 roi\n",
    "            scales,  # 第三个参数  scales\n",
    "            coordinate_transformation_mode_s=\"pytorch_half_pixel\",\n",
    "            cubic_coeff_a_f=-0.75,\n",
    "            mode_s=\"cubic\",\n",
    "            nearest_mode_s=\"floor\",\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "映射到 ONNX 的方法由一个算子的 `symbolic` 方法决定。`symbolic` 方法第一个参数必须是`g`，之后的参数是算子的自定义输入，和 `forward` 函数一样。ONNX 算子的具体定义由 `g.op` 实现。`g.op` 的每个参数都可以映射到 ONNX 中的算子属性：\n",
    "\n",
    "<div align=\"center\">\n",
    "  <img src=\"./assets/symbolic_onnx_op.png\" height=\"400\"/> </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3137538/3127156981.py:4: TracerWarning: Converting a tensor to a Python list might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  scales = scales.tolist()[-2:]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============= Diagnostic Run torch.onnx.export version 2.0.1+cu118 =============\n",
      "verbose: False, log level: Level.ERROR\n",
      "======================= 0 NONE 0 NOTE 0 WARNING 0 ERROR ========================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class StrangeSuperResolutionNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=9, padding=4)\n",
    "        self.conv2 = nn.Conv2d(64, 32, kernel_size=1, padding=0)  # 压缩channel\n",
    "        self.conv3 = nn.Conv2d(32, 3, kernel_size=5, padding=2)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x, upscale_factor):\n",
    "        out = NewInterpolate.apply(x, upscale_factor)\n",
    "        out = self.relu(self.conv1(out))\n",
    "        out = self.relu(self.conv2(out))\n",
    "        out = self.conv3(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "torch_model = StrangeSuperResolutionNet()\n",
    "torch_model.load_state_dict(state_dicts)\n",
    "torch_model.eval()\n",
    "\n",
    "\n",
    "upscale_factor = torch.tensor([1, 1, 3, 3], dtype=torch.float32)\n",
    "torch_output = (\n",
    "    torch_model(torch.from_numpy(input_image), upscale_factor).detach().numpy()\n",
    ")\n",
    "torch_output = np.squeeze(torch_output, 0)\n",
    "torch_output = np.clip(torch_output, 0, 255)\n",
    "torch_output = np.transpose(torch_output, (1, 2, 0)).astype(np.uint8)\n",
    "cv2.imwrite(\"./outputs/face_torch_3.png\", torch_output)\n",
    "\n",
    "with torch.no_grad():\n",
    "    torch.onnx.export(\n",
    "        model=torch_model,\n",
    "        args=(x, upscale_factor),\n",
    "        f=\"./outputs/srcnn3.onnx\",\n",
    "        opset_version=11,\n",
    "        input_names=[\"input\", \"factor\"],\n",
    "        output_names=[\"output\"],\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在我们已经成功导出来了新的ONXX模型，虽然上面依然有一条Tracing的Warning，但我们通过Netron.APP看到导出的ONNX算子，已经支持两个输入了。\n",
    "\n",
    "<div align=\"center\">\n",
    "  <img src=\"./assets/new_onnx_model.png\" height=\"400\"/> </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ort_session = onnxruntime.InferenceSession(\"./outputs/srcnn3.onnx\")\n",
    "\n",
    "ort_inputs = {\"input\": input_image, \"factor\": upscale_factor.numpy()}\n",
    "\n",
    "# 输入输出都是numpy.ndarry\n",
    "ort_output = ort_session.run(output_names=[\"output\"], input_feed=ort_inputs)[0]\n",
    "\n",
    "ort_output = np.squeeze(ort_output, 0)\n",
    "ort_output = np.clip(ort_output, 0, 255)\n",
    "ort_output = np.transpose(ort_output, (1, 2, 0)).astype(np.uint8)\n",
    "\n",
    "successed = cv2.imwrite(\"./outputs/face_ort_3.png\", ort_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK，导出的ONNX模型在ONNXRuntime中也能成功运行了；回顾一下，最开始不能直接导出的主要原因是，原始的`interpolate`算子的第二个参数是一个python number，它在向ONNX进行算子转换时，会被当作一个Çonstant进行转换。所以如果能把第二个参数改为一个`Tensor`就可以了，所以我们手动定义了一个WrapInterpolate函数，并通完了`symbolic`来手动定义它与ONNX算子之间的映射关系。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Pytorch模型转ONNX模型\n",
    "\n",
    "ONNX 是目前模型部署中最重要的中间表示之一。学懂了 ONNX 的技术细节，就能规避大量的模型部署问题。\n",
    "\n",
    "* 第一篇文章里，我们会介绍更多 PyTorch 转 ONNX 的细节，让大家完全掌握把简单的 PyTorch 模型转成 ONNX 模型的方法；\n",
    "* 在第二篇文章里，我们将介绍如何在 PyTorch 中支持更多的 ONNX 算子，让大家能彻底走通 PyTorch 到 ONNX 这条部署路线；\n",
    "* 第三篇文章里，我们讲介绍 ONNX 本身的知识，以及修改、调试 ONNX 模型的常用方法，使大家能自行解决大部分和 ONNX 有关的部署问题。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 `torch.onnx.export`详解\n",
    "\n",
    "在把 PyTorch 模型转换成 ONNX 模型时，我们往往只需要轻松地调用一句`torch.onnx.export`就行了。这个函数的接口看上去简单，但它在使用上还有着诸多的“潜规则”。\n",
    "\n",
    "`torch.onnx.export`把一个Pytorch Module转换为一个ONNX模型，实际是经过了2步：\n",
    "\n",
    "1. 先把动态图转换为`torch.jit.ScriptModule`。\n",
    "2. 再导出为ONNX模型。\n",
    "\n",
    "如果我们传给`torch.onnx.export`接口的是一个普通的`torch.nn.Module`，那么会先通过`torch.jit.trace`方式把模型变为`torch.jit.ScriptModule`。所以我们也可以直接给接口传入一个已经转好的`torch.jit.ScriptModule`。\n",
    "\n",
    "<div align=\"center\">\n",
    "  <img src=\"./assets/torch_onnx_export.drawio.svg\" width=\"800\"/> </div>\n",
    "\n",
    "\n",
    "我们在进行实际的模型部署时，不需要显式的把pytorch模型转换为TorchScript模型，直接调用`export`接口导出即可，但了解上面的过程，有利于我们在导出时遇到报错时，可以分析问题发生在哪一步。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============= Diagnostic Run torch.onnx.export version 2.0.1+cu118 =============\n",
      "verbose: False, log level: Level.ERROR\n",
      "======================= 0 NONE 0 NOTE 0 WARNING 0 ERROR ========================\n",
      "\n",
      "============= Diagnostic Run torch.onnx.export version 2.0.1+cu118 =============\n",
      "verbose: False, log level: Level.ERROR\n",
      "======================= 0 NONE 0 NOTE 0 WARNING 0 ERROR ========================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yangyansheng/miniconda3/envs/pyml/lib/python3.8/site-packages/torch/onnx/utils.py:825: UserWarning: no signature found for <torch.ScriptMethod object at 0x7f6067655a90>, skipping _decide_input_format\n",
      "  warnings.warn(f\"{e}, skipping _decide_input_format\")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class Model(torch.nn.Module):\n",
    "    def __init__(self, n):\n",
    "        super().__init__()\n",
    "        self.n = n\n",
    "        self.conv = nn.Conv2d(in_channels=3, out_channels=3, kernel_size=3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        for _ in range(self.n):\n",
    "            x = self.conv(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "model = Model(n=3)\n",
    "dummy_input = torch.randn(1, 3, 10, 10)\n",
    "trace_model = torch.jit.trace(model, example_inputs=dummy_input)\n",
    "torch.onnx.export(trace_model, [dummy_input], \"./outputs/trace_model.onnx\")\n",
    "\n",
    "script_model = torch.jit.script(model)\n",
    "torch.onnx.export(script_model, [dummy_input], \"./outputs/script_model.onnx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上面两种方式导出的ONNX模型可视化出现没有区别，但是按原文档，script model导出的模型，最终的 ONNX 模型用 Loop 节点来表示循环。不清楚是不是因为版本的原因导致的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面我们来从应用的角度来介绍每个参数在不同模型部署场景中该如何设置。\n",
    "\n",
    "```python\n",
    "def export(\n",
    "    # 要导出的模型，目前nn.Module和ScripModule\n",
    "    model: Union[torch.nn.Module, torch.jit.ScriptModule, torch.jit.ScriptFunction],\n",
    "    # 模型的输入，如果只有一个参数，则为Tensor，如果多个输入，则是一个Tuple\n",
    "    args: Union[Tuple[Any, ...], torch.Tensor],\n",
    "    # 导出的ONXX的文件路径，或者BytesIO\n",
    "    f: Union[str, io.BytesIO],\n",
    "    # 在导出模型结构时，是否同时导出模型参数，一般都设置为True\n",
    "    # False 当我们只为了在不同框架传递模型时，可能会用上，一般模型时用不上\n",
    "    export_params: bool = True,\n",
    "    verbose: bool = False,\n",
    "    training: _C_onnx.TrainingMode = _C_onnx.TrainingMode.EVAL,\n",
    "    # 设置输入和输出张量的名称。如果不设置的话，会自动分配一些简单的名字（如数字）。\n",
    "    # 很多推理框架在推理ONNX时，都会以{\"name\": Tensor}来作为输入\n",
    "    input_names: Optional[Sequence[str]] = None,\n",
    "    # 如 input_names\n",
    "    output_names: Optional[Sequence[str]] = None,\n",
    "    operator_export_type: _C_onnx.OperatorExportTypes = _C_onnx.OperatorExportTypes.ONNX,\n",
    "    opset_version: Optional[int] = None,\n",
    "    do_constant_folding: bool = True,\n",
    "    dynamic_axes: Optional[\n",
    "        Union[Mapping[str, Mapping[int, str]], Mapping[str, Sequence[int]]]\n",
    "    ] = None,\n",
    "    keep_initializers_as_inputs: Optional[bool] = None,\n",
    "    custom_opsets: Optional[Mapping[str, int]] = None,\n",
    "    export_modules_as_functions: Union[bool, Collection[Type[torch.nn.Module]]] = False,\n",
    ") -> None:\n",
    "```\n",
    "\n",
    "重要的几个参数，已经在上面接口里做了注释。另外两个重要参数，需要重点介绍一下：\n",
    "\n",
    "**dynamic_axes**\n",
    "\n",
    "指定输入输出张量的哪些维度是动态的。 为了追求效率，ONNX 默认所有参与运算的张量都是静态的（张量的形状不发生改变）。但在实际应用中，我们又希望模型的输入张量是动态的，尤其是本来就没有形状限制的全卷积模型。因此，我们需要显式地指明输入输出张量的哪几个维度的大小是可变的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============= Diagnostic Run torch.onnx.export version 2.0.1+cu118 =============\n",
      "verbose: False, log level: Level.ERROR\n",
      "======================= 0 NONE 0 NOTE 0 WARNING 0 ERROR ========================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yangyansheng/miniconda3/envs/pyml/lib/python3.8/site-packages/torch/onnx/utils.py:2033: UserWarning: No names were found for specified dynamic axes of provided input.Automatically generated names will be applied to each dynamic axes of input in\n",
      "  warnings.warn(\n",
      "/home/yangyansheng/miniconda3/envs/pyml/lib/python3.8/site-packages/torch/onnx/utils.py:2033: UserWarning: No names were found for specified dynamic axes of provided input.Automatically generated names will be applied to each dynamic axes of input out\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "class Model(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(in_channels=3, out_channels=3, kernel_size=3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "\n",
    "model = Model()\n",
    "dummy_input = torch.randn(1, 3, 10, 10)\n",
    "torch.onnx.export(\n",
    "    model,\n",
    "    dummy_input,\n",
    "    \"./outputs/dynamic_input_cnn.onnx\",\n",
    "    input_names=[\"in\"],\n",
    "    output_names=[\"out\"],\n",
    "    dynamic_axes={\"in\": [2, 3], \"out\": [2, 3]},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "由于 ONNX 要求每个动态维度都有一个名字，这样写的话会引出一条 UserWarning，警告我们通过列表的方式设置动态维度的话系统会自动为它们分配名字。一种显式添加动态维度名字的方法如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============= Diagnostic Run torch.onnx.export version 2.0.1+cu118 =============\n",
      "verbose: False, log level: Level.ERROR\n",
      "======================= 0 NONE 0 NOTE 0 WARNING 0 ERROR ========================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "torch.onnx.export(\n",
    "    model,\n",
    "    dummy_input,\n",
    "    \"./outputs/dynamic_input_cnn.onnx\",\n",
    "    input_names=[\"in\"],\n",
    "    output_names=[\"out\"],\n",
    "    dynamic_axes={\"in\": {2: \"height\", 3: \"width\"}, \"out\": {2: \"height\", 3: \"width\"}},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnxruntime\n",
    "import numpy as np\n",
    "\n",
    "ort_session = onnxruntime.InferenceSession(\"./outputs/dynamic_input_cnn.onnx\")\n",
    "img = np.random.rand(1, 3, 20, 20).astype(\"float32\")\n",
    "ort_inputs = {\"in\": img}\n",
    "ort_output = ort_session.run(output_names=[\"out\"], input_feed=ort_inputs)[0]\n",
    "\n",
    "img = np.random.rand(1, 3, 30, 30).astype(\"float32\")\n",
    "ort_inputs = {\"in\": img}\n",
    "ort_output = ort_session.run(output_names=[\"out\"], input_feed=ort_inputs)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在上面的代码里，我们把input的后2个维度改为了: `20*20`以及`30*30`，可以正常推理。但如果我们改变batch这个维度，则会报错。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidArgument",
     "evalue": "[ONNXRuntimeError] : 2 : INVALID_ARGUMENT : Got invalid dimensions for input: in for the following indices\n index: 0 Got: 2 Expected: 1\n Please fix either the inputs or the model.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgument\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m/home/yangyansheng/workspace/pyml/model-deployment/tutorial.ipynb Cell 42\u001b[0m line \u001b[0;36m3\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a225034313132227d/home/yangyansheng/workspace/pyml/model-deployment/tutorial.ipynb#X55sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m img \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39mrand(\u001b[39m2\u001b[39m, \u001b[39m3\u001b[39m, \u001b[39m20\u001b[39m, \u001b[39m20\u001b[39m)\u001b[39m.\u001b[39mastype(\u001b[39m\"\u001b[39m\u001b[39mfloat32\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a225034313132227d/home/yangyansheng/workspace/pyml/model-deployment/tutorial.ipynb#X55sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m ort_inputs \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39min\u001b[39m\u001b[39m\"\u001b[39m: img}\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a225034313132227d/home/yangyansheng/workspace/pyml/model-deployment/tutorial.ipynb#X55sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m ort_output \u001b[39m=\u001b[39m ort_session\u001b[39m.\u001b[39;49mrun(output_names\u001b[39m=\u001b[39;49m[\u001b[39m\"\u001b[39;49m\u001b[39mout\u001b[39;49m\u001b[39m\"\u001b[39;49m], input_feed\u001b[39m=\u001b[39;49mort_inputs)[\u001b[39m0\u001b[39m]\n",
      "File \u001b[0;32m~/miniconda3/envs/pyml/lib/python3.8/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py:217\u001b[0m, in \u001b[0;36mSession.run\u001b[0;34m(self, output_names, input_feed, run_options)\u001b[0m\n\u001b[1;32m    215\u001b[0m     output_names \u001b[39m=\u001b[39m [output\u001b[39m.\u001b[39mname \u001b[39mfor\u001b[39;00m output \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_outputs_meta]\n\u001b[1;32m    216\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 217\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sess\u001b[39m.\u001b[39;49mrun(output_names, input_feed, run_options)\n\u001b[1;32m    218\u001b[0m \u001b[39mexcept\u001b[39;00m C\u001b[39m.\u001b[39mEPFail \u001b[39mas\u001b[39;00m err:\n\u001b[1;32m    219\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_enable_fallback:\n",
      "\u001b[0;31mInvalidArgument\u001b[0m: [ONNXRuntimeError] : 2 : INVALID_ARGUMENT : Got invalid dimensions for input: in for the following indices\n index: 0 Got: 2 Expected: 1\n Please fix either the inputs or the model."
     ]
    }
   ],
   "source": [
    "img = np.random.rand(2, 3, 20, 20).astype(\"float32\")\n",
    "ort_inputs = {\"in\": img}\n",
    "ort_output = ort_session.run(output_names=[\"out\"], input_feed=ort_inputs)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上面的错误告诉我们，在一个`in`的输入上维度无效，预期是1，输入是2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 `torch.onnx.export`的使用技巧\n",
    "\n",
    "通过学习之前的知识，我们基本掌握了 torch.onnx.export 函数的部分实现原理和参数设置方法，足以完成简单模型的转换了。但在实际应用中，使用该函数还会踩很多坑。这里把一些在实战中积累的一些经验分享给大家。\n",
    "\n",
    "### 控制部分路径不导出到ONNX模型中\n",
    "\n",
    "有些时候，我们希望模型在直接用 PyTorch 推理时有一套逻辑，而在导出的ONNX模型中有另一套逻辑。比如，我们可以把一些后处理的逻辑放在模型里，以简化除运行模型之外的其他代码。但使用`torch.onnx.is_in_onnx_export()`也有一些缺点：\n",
    "\n",
    "使用 `is_in_onnx_export`确实能让我们方便地在代码中添加和模型部署相关的逻辑。但是，这些代码对只关心模型训练的开发者和用户来说很不友好，突兀的部署逻辑会降低代码整体的可读性。同时，`is_in_onnx_export`` 只能在每个需要添加部署逻辑的地方都“打补丁”，难以进行统一的管理。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "class Model(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv = torch.nn.Conv2d(3, 3, 3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        # 我们仅在模型导出时把输出张量的数值限制在[0, 1]之间。\n",
    "        if torch.onnx.is_in_onnx_export():\n",
    "            x = torch.clip(x, 0, 1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 利用张量中断来进行模型静态化\n",
    "\n",
    "在模型tracing时，一些对于Tensor的操作，比如`.item()`，对tensor进行遍历，通过List来创建Tensor，这些都会导致Tracing的中断，这些中间变量最终作为常量放在序列化模型中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============= Diagnostic Run torch.onnx.export version 2.0.1+cu118 =============\n",
      "verbose: False, log level: Level.ERROR\n",
      "======================= 0 NONE 0 NOTE 0 WARNING 0 ERROR ========================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4506/120990205.py:6: TracerWarning: Converting a tensor to a Python number might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  x = x * x[0].item()\n",
      "/tmp/ipykernel_4506/120990205.py:7: TracerWarning: Iterating over a tensor might cause the trace to be incorrect. Passing a tensor of different shape won't change the number of iterations executed (and might lead to errors or silently give incorrect results).\n",
      "  return x, torch.Tensor([i for i in x])\n",
      "/tmp/ipykernel_4506/120990205.py:7: TracerWarning: torch.Tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
      "  return x, torch.Tensor([i for i in x])\n",
      "/tmp/ipykernel_4506/120990205.py:7: TracerWarning: Converting a tensor to a Python float might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  return x, torch.Tensor([i for i in x])\n"
     ]
    }
   ],
   "source": [
    "class Model(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x * x[0].item()\n",
    "        return x, torch.Tensor([i for i in x])\n",
    "\n",
    "\n",
    "model = Model()\n",
    "dummy_input = torch.rand(10)\n",
    "torch.onnx.export(model, dummy_input, \"./outputs/a.onnx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Pytorch对于ONNX算子的支持\n",
    "\n",
    "\n",
    "在确保torch.onnx.export()的调用方法无误后，PyTorch 转 ONNX 时最容易出现的问题就是算子不兼容了。这里我们会介绍如何判断某个 PyTorch 算子在 ONNX 中是否兼容，以助大家在碰到报错时能更好地把错误归类。\n",
    "\n",
    "在转换普通的torch.nn.Module模型时，PyTorch 一方面会用跟踪法执行前向推理，把遇到的算子整合成计算图；另一方面，PyTorch 还会把遇到的每个算子翻译成 ONNX 中定义的算子。在这个翻译过程中，可能会碰到以下情况：\n",
    "\n",
    "* 该算子可以一对一地翻译成一个 ONNX 算子。\n",
    "* 该算子在 ONNX 中没有直接对应的算子，会翻译成一至多个 ONNX 算子。\n",
    "* 该算子没有定义翻译成 ONNX 的规则，报错。\n",
    "\n",
    "\n",
    "ONNX的算子表格：https://github.com/onnx/onnx/blob/main/docs/Operators.md，我们可以在这个表格上查到所有ONNX算子从哪个opset_version来始支持的，以及后续的变更记录。\n",
    "\n",
    "在Pytorch中，和ONNX有关的定义全部放在 torch.onxx 目录（https://github.com/pytorch/pytorch/tree/main/torch/onnx）中。其中，symbloic_opset{n}.py（符号表文件）即表示 PyTorch 在支持第 n 版 ONNX 算子集时新加入的内容。\n",
    "\n",
    "如果我们想知道某个pytorch算子如何被翻译为ONNX算子的，就可以在上面的目录中进行搜索，看是哪个某个对应的opset版本中对应的定义的变化。比如：对于`_interpolate`算子，我们可以看到在`symbolic_helper.py`中的定义如下：\n",
    "\n",
    "```python\n",
    "return g.op(\n",
    "            \"Resize\",\n",
    "            input,\n",
    "            empty_roi,\n",
    "            empty_scales,\n",
    "            size,\n",
    "            coordinate_transformation_mode_s=coordinate_transformation_mode,\n",
    "            cubic_coeff_a_f=-0.75,  # only valid when mode=\"cubic\"\n",
    "            mode_s=mode,  # nearest, linear, or cubic\n",
    "            nearest_mode_s=\"floor\",\n",
    "        )\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. 解决部署中的算子适配问题\n",
    "\n",
    "从以上pytorch模型转ONNX中可以看出，如果想要顺利转换，一般要满足以下的条件：\n",
    "\n",
    "1. 我们要转换的计算过程，在Pytorch中都有对应的算子实现。\n",
    "2. 存在方法把Pytorch算子映射成一个或多个ONNX算子\n",
    "3. ONNX也存在对应的算子实现相应的功能。\n",
    "\n",
    "可在实际部署中，这三部分的内容都可能有所缺失。其中最坏的情况是：我们定义了一个全新的算子，它不仅缺少 PyTorch 实现，还缺少 PyTorch 到 ONNX 的映射关系。我们可以考虑从以下三个方面，解决存在的问题：\n",
    "\n",
    "1. 对于Pytorch算子支持的问题，我们可以考虑通过：1）组合现有的算子；2）添加torchscript算子；3）添加普通C++扩展算子。\n",
    "2. 映射方面：1）为ATen算子添加符号函数；2）为Torchscript算子添加符号函数；3）封装成torch.autograd.Function并添加符号函数。\n",
    "3. ONNX算子支持方面：1）使用现有的ONNX算子；2）定义新的ONNX算子。\n",
    "\n",
    "那么，面对不同的情况时，就需要我们灵活地选用和组合这些方法。听起来是不是很复杂？别担心，本篇文章中，我们将围绕着三种算子映射方法，学习三个添加算子支持的实例，来理清如何为 PyTorch 算子转 ONNX 算子的三个环节添加支持。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 为Aten算子定义转换规则\n",
    "\n",
    "实际部署过程中，我们最常遇到的是，部分对Tensor的操作，在导出ONNX时，发现没有对应的映射。比如下面代码中，我们使用了`asinh`这个算子，它是一个ATen算子。而且这个算子在ONNX算子表中，从版本9开始就支持了。\n",
    "\n",
    "PS. ATen 是 PyTorch 内置的 C++ 张量计算库，PyTorch 算子在底层绝大多数计算都是用 ATen 实现的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============= Diagnostic Run torch.onnx.export version 2.0.1+cu118 =============\n",
      "verbose: False, log level: Level.ERROR\n",
      "======================= 0 NONE 0 NOTE 0 WARNING 0 ERROR ========================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "class Model(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.asinh(x)\n",
    "\n",
    "\n",
    "model = Model()\n",
    "dummpy_input = torch.randn(2, 3)\n",
    "torch.onnx.export(model, dummpy_input, \"./outputs/asinh.onnx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上面的代码在导出时报错了：\"Exporting the operator 'aten::asinh' to ONNX opset version 14 is not supported. \"，这就是典型的没有在Pytorch中定义算子的映射规则。\n",
    "\n",
    "那么我们可以就可以通过为Aten算子编写符号函数来指明映射规则。这里有几个要点：\n",
    "\n",
    "1. 找到Pytorch中`asinh`这个ATen算子的接口定义\n",
    "2. 查找对应的ONNX算子的名称与输入输出，建立对应。\n",
    "\n",
    "`asinh`的接口定义为：`def asinh(input: Tensor, *, out: Optional[Tensor]=None) -> Tensor: ...`\n",
    "\n",
    "我们定义的映射函数（符号函数）的接口要与`asinh`的接口完全一致。除了第一个参数是一个`torch._C.Graph`类型的参数`g`。在符号函数内部，通过`g.op`来返回一个ONNX中的算子定义。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============= Diagnostic Run torch.onnx.export version 2.0.1+cu118 =============\n",
      "verbose: False, log level: Level.ERROR\n",
      "======================= 0 NONE 0 NOTE 0 WARNING 0 ERROR ========================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from torch.onnx import register_custom_op_symbolic\n",
    "\n",
    "\n",
    "def asinh_symbolic(g, input, *, out=None):\n",
    "    # g.op的第一个参数是ONNX中算子的名称\n",
    "    # 然后依次为这个ONNX算子的输入参数\n",
    "    return g.op(\"Asinh\", input)\n",
    "\n",
    "\n",
    "# 向torch注意自定义的符号函数。在之前的版本中，这个函数为 register_op，新版本中已经找不到了\n",
    "# 第一个参数是一个 \"<domain>::<op>\"\n",
    "# 第三个参数指定了ONNX的算子版本\n",
    "register_custom_op_symbolic(\"::asinh\", asinh_symbolic, 9)\n",
    "\n",
    "torch.onnx.export(\n",
    "    model,\n",
    "    dummpy_input,\n",
    "    \"./outputs/asinh.onnx\",\n",
    "    input_names=[\"in\"],\n",
    "    output_names=[\"out\"],\n",
    "    opset_version=9,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "值得注意的是，这里向第 9 号算子集注册，不代表较新的算子集（第 10 号、第 11 号……）都得到了注册。在示例中，我们先只向第 9 号算子集注册。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.01566388, 0.3456329 , 0.6761582 ],\n",
       "       [0.14482886, 0.4115883 , 0.36688334]], dtype=float32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import onnxruntime\n",
    "import numpy as np\n",
    "\n",
    "ort_session = onnxruntime.InferenceSession(\"./outputs/asinh.onnx\")\n",
    "t = np.random.rand(2, 3).astype(\"float32\")\n",
    "ort_inputs = {\"in\": t}\n",
    "ort_session.run(output_names=[\"out\"], input_feed=ort_inputs)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 为自定义Torch算子转ONNX\n",
    "\n",
    "\n",
    "### 5.2.1 自定义的torchscript算子\n",
    "\n",
    "对于一些比较复杂的运算，仅使用 PyTorch 原生算子是无法实现的。这个时候，就要考虑自定义一个 PyTorch 算子，再把它转换到 ONNX 中了。新增 PyTorch 算子的方法有很多，PyTorch 官方比较推荐的一种做法是添加 C++的扩展TorchScript 算子 。\n",
    "\n",
    "由于用C++扩展一个支持torchScript算子比较复杂，现在我们可以用`torchvision`中的一个`DeformConv2d`来示例。`DeformConv2d`是torchvision中实现的一个算子扩展实现。\n",
    "\n",
    "最终的算子是：`torch.ops.torchvision.deform_conv2d`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yangyansheng/miniconda3/envs/pyml/lib/python3.8/site-packages/torch/onnx/utils.py:825: UserWarning: no signature found for <torch.ScriptMethod object at 0x7f9fe7a6f540>, skipping _decide_input_format\n",
      "  warnings.warn(f\"{e}, skipping _decide_input_format\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============= Diagnostic Run torch.onnx.export version 2.0.1+cu118 =============\n",
      "verbose: False, log level: Level.ERROR\n",
      "======================= 0 NONE 0 NOTE 0 WARNING 1 ERROR ========================\n",
      "ERROR: missing-custom-symbolic-function\n",
      "=======================================\n",
      "ONNX export failed on an operator with unrecognized namespace torchvision::deform_conv2d. If you are trying to export a custom operator, make sure you registered it with the right domain and version.\n",
      "None\n",
      "<Set verbose=True to see more details>\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "UnsupportedOperatorError",
     "evalue": "ONNX export failed on an operator with unrecognized namespace torchvision::deform_conv2d. If you are trying to export a custom operator, make sure you registered it with the right domain and version.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnsupportedOperatorError\u001b[0m                  Traceback (most recent call last)",
      "\u001b[1;32m/home/yangyansheng/workspace/pyml/model-deployment/tutorial.ipynb Cell 57\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a225034313132227d/home/yangyansheng/workspace/pyml/model-deployment/tutorial.ipynb#Y111sdnNjb2RlLXJlbW90ZQ%3D%3D?line=15'>16</a>\u001b[0m dummpy_input \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mrandn(\u001b[39m1\u001b[39m, \u001b[39m3\u001b[39m, \u001b[39m32\u001b[39m, \u001b[39m32\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a225034313132227d/home/yangyansheng/workspace/pyml/model-deployment/tutorial.ipynb#Y111sdnNjb2RlLXJlbW90ZQ%3D%3D?line=16'>17</a>\u001b[0m trace_model \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mjit\u001b[39m.\u001b[39mtrace(model, [dummpy_input])\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a225034313132227d/home/yangyansheng/workspace/pyml/model-deployment/tutorial.ipynb#Y111sdnNjb2RlLXJlbW90ZQ%3D%3D?line=17'>18</a>\u001b[0m torch\u001b[39m.\u001b[39;49monnx\u001b[39m.\u001b[39;49mexport(trace_model, dummpy_input, \u001b[39m\"\u001b[39;49m\u001b[39m./outputs/deform_conv2d.onnx\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[0;32m~/miniconda3/envs/pyml/lib/python3.8/site-packages/torch/onnx/utils.py:506\u001b[0m, in \u001b[0;36mexport\u001b[0;34m(model, args, f, export_params, verbose, training, input_names, output_names, operator_export_type, opset_version, do_constant_folding, dynamic_axes, keep_initializers_as_inputs, custom_opsets, export_modules_as_functions)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[39m@_beartype\u001b[39m\u001b[39m.\u001b[39mbeartype\n\u001b[1;32m    189\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mexport\u001b[39m(\n\u001b[1;32m    190\u001b[0m     model: Union[torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mModule, torch\u001b[39m.\u001b[39mjit\u001b[39m.\u001b[39mScriptModule, torch\u001b[39m.\u001b[39mjit\u001b[39m.\u001b[39mScriptFunction],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    206\u001b[0m     export_modules_as_functions: Union[\u001b[39mbool\u001b[39m, Collection[Type[torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mModule]]] \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    207\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    208\u001b[0m \u001b[39m    \u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"Exports a model into ONNX format.\u001b[39;00m\n\u001b[1;32m    209\u001b[0m \n\u001b[1;32m    210\u001b[0m \u001b[39m    If ``model`` is not a :class:`torch.jit.ScriptModule` nor a\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    503\u001b[0m \u001b[39m            All errors are subclasses of :class:`errors.OnnxExporterError`.\u001b[39;00m\n\u001b[1;32m    504\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 506\u001b[0m     _export(\n\u001b[1;32m    507\u001b[0m         model,\n\u001b[1;32m    508\u001b[0m         args,\n\u001b[1;32m    509\u001b[0m         f,\n\u001b[1;32m    510\u001b[0m         export_params,\n\u001b[1;32m    511\u001b[0m         verbose,\n\u001b[1;32m    512\u001b[0m         training,\n\u001b[1;32m    513\u001b[0m         input_names,\n\u001b[1;32m    514\u001b[0m         output_names,\n\u001b[1;32m    515\u001b[0m         operator_export_type\u001b[39m=\u001b[39;49moperator_export_type,\n\u001b[1;32m    516\u001b[0m         opset_version\u001b[39m=\u001b[39;49mopset_version,\n\u001b[1;32m    517\u001b[0m         do_constant_folding\u001b[39m=\u001b[39;49mdo_constant_folding,\n\u001b[1;32m    518\u001b[0m         dynamic_axes\u001b[39m=\u001b[39;49mdynamic_axes,\n\u001b[1;32m    519\u001b[0m         keep_initializers_as_inputs\u001b[39m=\u001b[39;49mkeep_initializers_as_inputs,\n\u001b[1;32m    520\u001b[0m         custom_opsets\u001b[39m=\u001b[39;49mcustom_opsets,\n\u001b[1;32m    521\u001b[0m         export_modules_as_functions\u001b[39m=\u001b[39;49mexport_modules_as_functions,\n\u001b[1;32m    522\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/pyml/lib/python3.8/site-packages/torch/onnx/utils.py:1548\u001b[0m, in \u001b[0;36m_export\u001b[0;34m(model, args, f, export_params, verbose, training, input_names, output_names, operator_export_type, export_type, opset_version, do_constant_folding, dynamic_axes, keep_initializers_as_inputs, fixed_batch_size, custom_opsets, add_node_names, onnx_shape_inference, export_modules_as_functions)\u001b[0m\n\u001b[1;32m   1545\u001b[0m     dynamic_axes \u001b[39m=\u001b[39m {}\n\u001b[1;32m   1546\u001b[0m _validate_dynamic_axes(dynamic_axes, model, input_names, output_names)\n\u001b[0;32m-> 1548\u001b[0m graph, params_dict, torch_out \u001b[39m=\u001b[39m _model_to_graph(\n\u001b[1;32m   1549\u001b[0m     model,\n\u001b[1;32m   1550\u001b[0m     args,\n\u001b[1;32m   1551\u001b[0m     verbose,\n\u001b[1;32m   1552\u001b[0m     input_names,\n\u001b[1;32m   1553\u001b[0m     output_names,\n\u001b[1;32m   1554\u001b[0m     operator_export_type,\n\u001b[1;32m   1555\u001b[0m     val_do_constant_folding,\n\u001b[1;32m   1556\u001b[0m     fixed_batch_size\u001b[39m=\u001b[39;49mfixed_batch_size,\n\u001b[1;32m   1557\u001b[0m     training\u001b[39m=\u001b[39;49mtraining,\n\u001b[1;32m   1558\u001b[0m     dynamic_axes\u001b[39m=\u001b[39;49mdynamic_axes,\n\u001b[1;32m   1559\u001b[0m )\n\u001b[1;32m   1561\u001b[0m \u001b[39m# TODO: Don't allocate a in-memory string for the protobuf\u001b[39;00m\n\u001b[1;32m   1562\u001b[0m defer_weight_export \u001b[39m=\u001b[39m (\n\u001b[1;32m   1563\u001b[0m     export_type \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m _exporter_states\u001b[39m.\u001b[39mExportTypes\u001b[39m.\u001b[39mPROTOBUF_FILE\n\u001b[1;32m   1564\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/pyml/lib/python3.8/site-packages/torch/onnx/utils.py:1117\u001b[0m, in \u001b[0;36m_model_to_graph\u001b[0;34m(model, args, verbose, input_names, output_names, operator_export_type, do_constant_folding, _disable_torch_constant_prop, fixed_batch_size, training, dynamic_axes)\u001b[0m\n\u001b[1;32m   1114\u001b[0m params_dict \u001b[39m=\u001b[39m _get_named_param_dict(graph, params)\n\u001b[1;32m   1116\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1117\u001b[0m     graph \u001b[39m=\u001b[39m _optimize_graph(\n\u001b[1;32m   1118\u001b[0m         graph,\n\u001b[1;32m   1119\u001b[0m         operator_export_type,\n\u001b[1;32m   1120\u001b[0m         _disable_torch_constant_prop\u001b[39m=\u001b[39;49m_disable_torch_constant_prop,\n\u001b[1;32m   1121\u001b[0m         fixed_batch_size\u001b[39m=\u001b[39;49mfixed_batch_size,\n\u001b[1;32m   1122\u001b[0m         params_dict\u001b[39m=\u001b[39;49mparams_dict,\n\u001b[1;32m   1123\u001b[0m         dynamic_axes\u001b[39m=\u001b[39;49mdynamic_axes,\n\u001b[1;32m   1124\u001b[0m         input_names\u001b[39m=\u001b[39;49minput_names,\n\u001b[1;32m   1125\u001b[0m         module\u001b[39m=\u001b[39;49mmodule,\n\u001b[1;32m   1126\u001b[0m     )\n\u001b[1;32m   1127\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m   1128\u001b[0m     torch\u001b[39m.\u001b[39monnx\u001b[39m.\u001b[39mlog(\u001b[39m\"\u001b[39m\u001b[39mTorch IR graph at exception: \u001b[39m\u001b[39m\"\u001b[39m, graph)\n",
      "File \u001b[0;32m~/miniconda3/envs/pyml/lib/python3.8/site-packages/torch/onnx/utils.py:665\u001b[0m, in \u001b[0;36m_optimize_graph\u001b[0;34m(graph, operator_export_type, _disable_torch_constant_prop, fixed_batch_size, params_dict, dynamic_axes, input_names, module)\u001b[0m\n\u001b[1;32m    662\u001b[0m     _C\u001b[39m.\u001b[39m_jit_pass_onnx_set_dynamic_input_shape(graph, dynamic_axes, input_names)\n\u001b[1;32m    663\u001b[0m _C\u001b[39m.\u001b[39m_jit_pass_onnx_lint(graph)\n\u001b[0;32m--> 665\u001b[0m graph \u001b[39m=\u001b[39m _C\u001b[39m.\u001b[39;49m_jit_pass_onnx(graph, operator_export_type)\n\u001b[1;32m    666\u001b[0m _C\u001b[39m.\u001b[39m_jit_pass_onnx_lint(graph)\n\u001b[1;32m    667\u001b[0m _C\u001b[39m.\u001b[39m_jit_pass_lint(graph)\n",
      "File \u001b[0;32m~/miniconda3/envs/pyml/lib/python3.8/site-packages/torch/onnx/utils.py:1901\u001b[0m, in \u001b[0;36m_run_symbolic_function\u001b[0;34m(graph, block, node, inputs, env, operator_export_type)\u001b[0m\n\u001b[1;32m   1897\u001b[0m     \u001b[39mif\u001b[39;00m namespace \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39monnx\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m   1898\u001b[0m         \u001b[39m# Clone node to trigger ONNX shape inference\u001b[39;00m\n\u001b[1;32m   1899\u001b[0m         \u001b[39mreturn\u001b[39;00m graph_context\u001b[39m.\u001b[39mop(op_name, \u001b[39m*\u001b[39minputs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mattrs, outputs\u001b[39m=\u001b[39mnode\u001b[39m.\u001b[39moutputsSize())  \u001b[39m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[0;32m-> 1901\u001b[0m     \u001b[39mraise\u001b[39;00m errors\u001b[39m.\u001b[39mUnsupportedOperatorError(\n\u001b[1;32m   1902\u001b[0m         symbolic_function_name,\n\u001b[1;32m   1903\u001b[0m         opset_version,\n\u001b[1;32m   1904\u001b[0m         symbolic_function_group\u001b[39m.\u001b[39mget_min_supported()\n\u001b[1;32m   1905\u001b[0m         \u001b[39mif\u001b[39;00m symbolic_function_group\n\u001b[1;32m   1906\u001b[0m         \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   1907\u001b[0m     )\n\u001b[1;32m   1909\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m:\n\u001b[1;32m   1910\u001b[0m     \u001b[39mif\u001b[39;00m operator_export_type \u001b[39m==\u001b[39m _C_onnx\u001b[39m.\u001b[39mOperatorExportTypes\u001b[39m.\u001b[39mONNX_FALLTHROUGH:\n",
      "\u001b[0;31mUnsupportedOperatorError\u001b[0m: ONNX export failed on an operator with unrecognized namespace torchvision::deform_conv2d. If you are trying to export a custom operator, make sure you registered it with the right domain and version."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "\n",
    "\n",
    "class Model(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = torch.nn.Conv2d(3, 18, 3)\n",
    "        self.conv2 = torchvision.ops.DeformConv2d(3, 3, 3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv2(x, self.conv1(x))\n",
    "\n",
    "\n",
    "model = Model()\n",
    "dummpy_input = torch.randn(1, 3, 32, 32)\n",
    "# 可以trace成功\n",
    "trace_model = torch.jit.trace(model, [dummpy_input])\n",
    "# 但导出ONNX失败\n",
    "torch.onnx.export(trace_model, dummpy_input, \"./outputs/deform_conv2d.onnx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "也就是相当于现在C++扩展的算子已经写好了，需要把它映射到ONNX算子，按前面逻辑，我们只需要写符号函数，然后注册就可以了。这里遇到的问题是，我们发现ONNX目前也并不支持`DeformConv`，这就需要我们来自定义ONNX算子定义了。\n",
    "\n",
    "我们在前面讲过，`g.op()` 是用来定义 ONNX 算子的函数。对于 ONNX 官方定义的算子，`g.op()` 的第一个参数就是该算子的名称。而对于一个自定义算子，`g.op()` 的第一个参数是一个带命名空间的算子名，比如：\n",
    "\n",
    "```python\n",
    "g.op(\"custom::deform_conv2d, ...)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============= Diagnostic Run torch.onnx.export version 2.0.1+cu118 =============\n",
      "verbose: False, log level: Level.ERROR\n",
      "======================= 0 NONE 0 NOTE 1 WARNING 0 ERROR ========================\n",
      "1 WARNING were not printed due to the log level.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W shape_type_inference.cpp:1920] Warning: The shape inference of custom::deform_conv2d type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)\n"
     ]
    }
   ],
   "source": [
    "from torch.onnx import register_custom_op_symbolic\n",
    "from torch.onnx.symbolic_helper import parse_args\n",
    "\n",
    "\n",
    "# parse_args标注每个参数的类型，这个是对于torchscript算子来说，符号函数要要求的\n",
    "@parse_args(\"v\", \"v\", \"v\", \"v\", \"v\", \"i\", \"i\", \"i\", \"i\", \"i\", \"i\", \"i\", \"i\", \"none\")\n",
    "def symbolic(\n",
    "    g,\n",
    "    input,\n",
    "    weight,\n",
    "    offset,\n",
    "    mask,\n",
    "    bias,\n",
    "    stride_h,\n",
    "    stride_w,\n",
    "    pad_h,\n",
    "    pad_w,\n",
    "    dil_h,\n",
    "    dil_w,\n",
    "    n_weight_grps,\n",
    "    n_offset_grps,\n",
    "    use_mask,\n",
    "):\n",
    "    # 这里为了简单，只用Input和offset构造了一个简单的ONNX算子\n",
    "    return g.op(\"custom::deform_conv2d\", input, offset)\n",
    "\n",
    "\n",
    "register_custom_op_symbolic(\"torchvision::deform_conv2d\", symbolic, 9)\n",
    "torch.onnx.export(model, dummpy_input, \"./outputs/deform_conv2d.onnx\", opset_version=9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "其中，”::”前面的内容就是我们的命名空间。该概念和 C++ 的命名空间类似，是为了防止命名冲突而设定的。如果在 g.op() 里不加前面的命名空间，则算子会被默认成 ONNX 的官方算子。\n",
    "\n",
    "PyTorch 在运行 g.op() 时会对官方的算子做检查，如果算子名有误，或者算子的输入类型不正确， g.op() 就会报错。为了让我们随心所欲地定义新 ONNX 算子，我们必须设定一个命名空间，给算子取个名，再定义自己的算子。\n",
    "\n",
    "ONNX 是一套标准，本身并不包括实现。在这里，我们就简略地定义一个 ONNX 可变形卷积算子，而不去写它在某个推理引擎上的实现。\n",
    "\n",
    "\n",
    "这段代码中，最令人疑惑的就是装饰器 `@parse_args` 了。简单来说，TorchScript 算子的符号函数要求标注出每一个输入参数的类型。比如”v”表示 Torch 库里的 value 类型，一般用于标注张量，而”i”表示 int 类型，”f”表示 float 类型，”none”表示该参数为空。具体的类型含义可以在 `torch.onnx.symbolic_helper.py`中查看。这里输入参数中的 input, weight, offset, mask, bias 都是张量，所以用”v”表示。后面的其他参数同理。我们不必纠结于 @parse_args的原理，根据实际情况对符号函数的参数标注类型即可。\n",
    "\n",
    "\n",
    "### 5.2.2 自定义普通C++算子\n",
    "\n",
    "对于非torchscript的C++算子，我们可以直接映射到python接口，然后放在`torch.autograd.Function`来调用。那么可以直接在自定义的`torch.autograd.Function`里定义`symbolic`符号函数来定义与ONNX算子的映射。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyAddFunction(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, a, b):\n",
    "        return my_lib.my_add(a, b)\n",
    "\n",
    "    @staticmethod\n",
    "    def symbolic(g, a, b):\n",
    "        two = g.op(\"Constant\", value_t=torch.tensor([2]))\n",
    "        a = g.op(\"Mul\", a, two)\n",
    "        return g.op(\"Add\", a, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. 修改和调试ONNX模型\n",
    "\n",
    "我们有没有可能不用深度学习框架，自己用ONNX的API手动构造一个ONNX模型呢？或者当我们只拿到了一个ONNX模型时，我们如何对这个模型进行调试呢？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1 ONNX底层存储协议\n",
    "\n",
    "ONNX底层使用Protobuf来序列化模型。我们可以在[ONNX源码](https://github.com/onnx/onnx/tree/main/onnx)下面看到一些.proto文件。神经网络本质上是一个计算图。计算图的节点是算子，边是参与运算的张量。而通过可视化 ONNX 模型，我们知道 ONNX 记录了所有算子节点的属性信息，并把参与运算的张量信息存储在算子节点的输入输出信息中。事实上，ONNX 模型的结构可以用类图大致表示如下：\n",
    "\n",
    "\n",
    "<div align=\"center\">\n",
    "  <img src=\"./assets/onnx_proto.jpg\" height=\"500\"/> </div>\n",
    "\n",
    "一个 ONNX 模型可以用 `ModelProto` 类表示。`ModelProto` 包含了版本、创建者等日志信息，还包含了存储计算图结构的 `graph`。`GraphProto` 类则由输入张量信息、输出张量信息、节点信息组成。张量信息 `ValueInfoProto` 类包括张量名、基本数据类型、形状。节点信息 `NodeProto` 类包含了算子名、算子输入张量名、算子输出张量名。 \n",
    "\n",
    "下面我们使用ONNX API来构建一个`a * x + b`的模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ir_version: 9\n",
      "opset_import {\n",
      "  version: 19\n",
      "}\n",
      "graph {\n",
      "  node {\n",
      "    input: \"a\"\n",
      "    input: \"x\"\n",
      "    output: \"c\"\n",
      "    op_type: \"Mul\"\n",
      "  }\n",
      "  node {\n",
      "    input: \"c\"\n",
      "    input: \"b\"\n",
      "    output: \"output\"\n",
      "    op_type: \"Add\"\n",
      "  }\n",
      "  name: \"linear_func\"\n",
      "  input {\n",
      "    name: \"a\"\n",
      "    type {\n",
      "      tensor_type {\n",
      "        elem_type: 1\n",
      "        shape {\n",
      "          dim {\n",
      "            dim_value: 10\n",
      "          }\n",
      "          dim {\n",
      "            dim_value: 10\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  input {\n",
      "    name: \"x\"\n",
      "    type {\n",
      "      tensor_type {\n",
      "        elem_type: 1\n",
      "        shape {\n",
      "          dim {\n",
      "            dim_value: 10\n",
      "          }\n",
      "          dim {\n",
      "            dim_value: 10\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  input {\n",
      "    name: \"b\"\n",
      "    type {\n",
      "      tensor_type {\n",
      "        elem_type: 1\n",
      "        shape {\n",
      "          dim {\n",
      "            dim_value: 10\n",
      "          }\n",
      "          dim {\n",
      "            dim_value: 10\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  output {\n",
      "    name: \"output\"\n",
      "    type {\n",
      "      tensor_type {\n",
      "        elem_type: 1\n",
      "        shape {\n",
      "          dim {\n",
      "            dim_value: 10\n",
      "          }\n",
      "          dim {\n",
      "            dim_value: 10\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import onnx\n",
    "from onnx import helper\n",
    "from onnx import TensorProto\n",
    "\n",
    "# input and output Value\n",
    "a = helper.make_tensor_value_info(\"a\", TensorProto.FLOAT, [10, 10])\n",
    "x = helper.make_tensor_value_info(\"x\", TensorProto.FLOAT, [10, 10])\n",
    "b = helper.make_tensor_value_info(\"b\", TensorProto.FLOAT, [10, 10])\n",
    "output = helper.make_tensor_value_info(\"output\", TensorProto.FLOAT, [10, 10])\n",
    "\n",
    "# Mul node\n",
    "mul = helper.make_node(\"Mul\", [\"a\", \"x\"], [\"c\"])\n",
    "# Add node\n",
    "add = helper.make_node(\"Add\", [\"c\", \"b\"], [\"output\"])\n",
    "\n",
    "# Build graph\n",
    "# 这里 make_graph 的节点参数有一个要求：计算图的节点必须以拓扑序给出。\n",
    "graph = helper.make_graph([mul, add], \"linear_func\", [a, x, b], [output])\n",
    "model = helper.make_model(graph)\n",
    "\n",
    "# save onnx model\n",
    "onnx.checker.check_model(model)\n",
    "print(model)\n",
    "onnx.save(model, \"./outputs/linear_func.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnxruntime\n",
    "import numpy as np\n",
    "\n",
    "ort_session = onnxruntime.InferenceSession(\"./outputs/linear_func.onnx\")\n",
    "a = np.random.randn(10, 10).astype(np.float32)\n",
    "b = np.random.randn(10, 10).astype(np.float32)\n",
    "x = np.random.randn(10, 10).astype(np.float32)\n",
    "output = ort_session.run([\"output\"], {\"a\": a, \"b\": b, \"x\": x})[0]\n",
    "assert np.allclose(output, a * x + b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2 ONNX模型的读取与修改"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx\n",
    "\n",
    "model = onnx.load(\"./outputs/linear_func.onnx\")\n",
    "# 获取模型中所有node\n",
    "nodes = model.graph.node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "input: \"a\"\n",
       "input: \"x\"\n",
       "output: \"c\"\n",
       "op_type: \"Mul\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nodes[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "name: \"a\"\n",
       "type {\n",
       "  tensor_type {\n",
       "    elem_type: 1\n",
       "    shape {\n",
       "      dim {\n",
       "        dim_value: 10\n",
       "      }\n",
       "      dim {\n",
       "        dim_value: 10\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = model.graph.input\n",
    "inputs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将模型改为 a * x - b\n",
    "model.graph.node[1].op_type = \"Sub\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.3 ONNX模型的调试 - 子图提取\n",
    "\n",
    "在实际部署中，如果用深度学习框架导出的 ONNX 模型出了问题，一般要通过修改框架的代码来解决，而不会从 ONNX 入手，我们把 ONNX 模型当成一个不可修改的黑盒看待。 现在，我们已经深入学习了 ONNX 的原理，可以尝试对 ONNX 模型本身进行调试了。\n",
    "\n",
    "<div align=\"center\">\n",
    "  <img src=\"./assets/whole_model.png\" height=\"800\"/> </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============= Diagnostic Run torch.onnx.export version 2.0.1+cu118 =============\n",
      "verbose: False, log level: Level.ERROR\n",
      "======================= 0 NONE 0 NOTE 0 WARNING 0 ERROR ========================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "class Model(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.convs1 = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(3, 3, 3), torch.nn.Conv2d(3, 3, 3), torch.nn.Conv2d(3, 3, 3)\n",
    "        )\n",
    "        self.convs2 = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(3, 3, 3), torch.nn.Conv2d(3, 3, 3)\n",
    "        )\n",
    "        self.convs3 = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(3, 3, 3), torch.nn.Conv2d(3, 3, 3)\n",
    "        )\n",
    "        self.convs4 = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(3, 3, 3), torch.nn.Conv2d(3, 3, 3), torch.nn.Conv2d(3, 3, 3)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.convs1(x)\n",
    "        x1 = self.convs2(x)\n",
    "        x2 = self.convs3(x)\n",
    "        x = x1 + x2\n",
    "        x = self.convs4(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "model = Model()\n",
    "input = torch.randn(1, 3, 20, 20)\n",
    "\n",
    "torch.onnx.export(model, input, \"./outputs/whole_model.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx\n",
    "\n",
    "onnx.utils.extract_model(\n",
    "    \"./outputs/whole_model.onnx\",\n",
    "    \"./outputs/partial_model.onnx\",\n",
    "    input_names=[\"/convs1/convs1.2/Conv_output_0\"],\n",
    "    output_names=[\"/convs4/convs4.0/Conv_output_0\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "  <img src=\"./assets/partial_model.png\" height=\"500\"/> </div>\n",
    "\n",
    "子模型提取的实现原理：新建一个模型，把给定的输入和输出填入。之后把图的所有有向边反向，从输出边开始遍历节点，碰到输入边则停止，把这样遍历得到的节点做为子模型的节点。\n",
    "\n",
    "所以如果我们指定的输出在计算时，所依赖到的输入，在整个计算路径上有一些未列入input_names的依赖，则会失败。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.4 ONNX模型的调试 - 添加中间输出\n",
    "\n",
    "我们依然利用`extract_model`接口，只需要在output_names里添加我们要输出的Tensor的名字即可。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx\n",
    "\n",
    "onnx.utils.extract_model(\n",
    "    \"./outputs/whole_model.onnx\",\n",
    "    \"./outputs/multi_output_model.onnx\",\n",
    "    input_names=[\"/convs1/convs1.2/Conv_output_0\"],\n",
    "    output_names=[\n",
    "        \"/convs4/convs4.0/Conv_output_0\",\n",
    "        \"/convs3/convs3.1/Conv_output_0\",\n",
    "        \"/convs2/convs2.1/Conv_output_0\",\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "  <img src=\"./assets/multioutput_model.png\" height=\"500\"/> </div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
